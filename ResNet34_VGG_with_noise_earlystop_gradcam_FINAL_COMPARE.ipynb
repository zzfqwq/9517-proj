{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ccc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¶…å‚æ•°è®¾ç½®\n",
    "config = {\n",
    "    \"model_name\": \"resnet\",  # å¯é€‰ \"resnet\" æˆ– \"vgg\"\n",
    "    \"num_epochs\": 15,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"test_size\": 0.2,\n",
    "    \"sample_ratio\": 1.0\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_path = \"Aerial_Landscapes/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfaf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, use_pretrained=True):\n",
    "    input_size = 224\n",
    "    if model_name == \"resnet\":\n",
    "        model = models.resnet34(pretrained=use_pretrained)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)  # ä½¿ç”¨ResNet34\n",
    "    elif model_name == \"vgg\":\n",
    "        model = models.vgg16(pretrained=use_pretrained)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"ä¸æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œè¯·é€‰æ‹© 'resnet' æˆ– 'vgg'\")\n",
    "    return model, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(input_size=224):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_transform, test_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564eb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(root_dir, test_size=0.2, sample_ratio=1.0):\n",
    "    classes = sorted(os.listdir(root_dir))\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    train_images, train_labels = [], []\n",
    "    test_images, test_labels = [], []\n",
    "\n",
    "    for cls_name in classes:\n",
    "        cls_path = os.path.join(root_dir, cls_name)\n",
    "        img_files = [os.path.join(cls_path, f) for f in os.listdir(cls_path) if f.endswith('.jpg')]\n",
    "        random.seed(42)\n",
    "        random.shuffle(img_files)\n",
    "        n_samples = int(len(img_files) * sample_ratio)\n",
    "        split = int(n_samples * (1 - test_size))\n",
    "        imgs = img_files[:n_samples]\n",
    "        train_imgs = imgs[:split]\n",
    "        test_imgs = imgs[split:]\n",
    "        train_images.extend([cv2.imread(p) for p in train_imgs])\n",
    "        train_labels.extend([class_to_idx[cls_name]] * len(train_imgs))\n",
    "        test_images.extend([cv2.imread(p) for p in test_imgs])\n",
    "        test_labels.extend([class_to_idx[cls_name]] * len(test_imgs))\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels), classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc41c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, device, train_loader, test_loader, criterion, optimizer,\n",
    "                num_epochs=25, checkpoint_path='checkpoint.pth', patience=5):\n",
    "    \"\"\"\n",
    "    æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ŒåŠ å…¥ Early Stopping å’Œå†å²è®°å½•\n",
    "    \"\"\"\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'test_loss': [], 'test_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for batch in train_loader_tqdm:\n",
    "            # TODO: move your inner training loop here\n",
    "            if early_stop:\n",
    "                print(f\"âš ï¸ æ—©åœè§¦å‘äºç¬¬ {epoch+1} è½®\")\n",
    "                break\n",
    "\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += torch.sum(preds == labels.data)\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        epoch_train_loss = train_loss / total_train\n",
    "        epoch_train_acc = correct_train.double() / total_train\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc.item())\n",
    "\n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        model.eval()\n",
    "        test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_test += torch.sum(preds == labels.data)\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        epoch_test_loss = test_loss / total_test\n",
    "        epoch_test_acc = correct_test.double() / total_test\n",
    "        history['test_loss'].append(epoch_test_loss)\n",
    "        history['test_acc'].append(epoch_test_acc.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={epoch_train_acc:.4f}, Test Acc={epoch_test_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping åˆ¤æ–­\n",
    "        if epoch_test_acc > best_acc:\n",
    "            best_acc = epoch_test_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                early_stop = True\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(xticks_rotation='vertical', cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ef615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "(train_images, train_labels), (test_images, test_labels), classes = load_and_split_dataset(\n",
    "    dataset_path,\n",
    "    test_size=config[\"test_size\"],\n",
    "    sample_ratio=config[\"sample_ratio\"]\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "model, input_size = initialize_model(\n",
    "    model_name=config[\"model_name\"],\n",
    "    num_classes=len(classes),\n",
    "    use_pretrained=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "train_transform, test_transform = get_transforms(input_size)\n",
    "\n",
    "# æ„å»ºDatasetå’ŒLoader\n",
    "train_dataset = CustomDataset(train_images, train_labels, train_transform)\n",
    "test_dataset = CustomDataset(test_images, test_labels, test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model, history = train_model(\n",
    "    model, device, train_loader, test_loader,\n",
    "    criterion, optimizer,\n",
    "    num_epochs=config[\"num_epochs\"],\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "# è¯„ä¼°\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# è¾“å‡ºä¸‰ç§æŒ‡æ ‡ï¼šprecisionã€recallã€f1-score\n",
    "report = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "print(\"Evaluation Metrics (Precision, Recall, F1-score):\\n\")\n",
    "print(report)\n",
    "\n",
    "# æ··æ·†çŸ©é˜µå¯è§†åŒ–\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "plt.figure(figsize=(12, 10))\n",
    "disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(model_name, classes):\n",
    "    # åŠ è½½æ•°æ®\n",
    "    (train_images, train_labels), (test_images, test_labels), _ = load_and_split_dataset(\n",
    "        dataset_path,\n",
    "        test_size=config[\"test_size\"],\n",
    "        sample_ratio=config[\"sample_ratio\"]\n",
    "    )\n",
    "\n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    model, input_size = initialize_model(\n",
    "        model_name=model_name,\n",
    "        num_classes=len(classes),\n",
    "        use_pretrained=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # æ•°æ®å¢å¼º\n",
    "    train_transform, test_transform = get_transforms(input_size)\n",
    "\n",
    "    # æ„å»ºæ•°æ®é›†ä¸åŠ è½½å™¨\n",
    "    train_dataset = CustomDataset(train_images, train_labels, train_transform)\n",
    "    test_dataset = CustomDataset(test_images, test_labels, test_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    model, _ = train_model(\n",
    "        model, device, train_loader, test_loader,\n",
    "        criterion, optimizer,\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # è¯„ä¼°æŒ‡æ ‡ä¸æ··æ·†çŸ©é˜µ\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "    print(f\"===== {model_name.upper()} Evaluation Metrics =====\\n\")\n",
    "    print(report)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for {model_name.upper()}\")\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒæ—¶æ¯”è¾ƒ ResNet å’Œ VGG\n",
    "print(\"ğŸ” æ­£åœ¨æ¯”è¾ƒ ResNet34 ä¸ VGG16 çš„åˆ†ç±»æ€§èƒ½...\\n\")\n",
    "run_and_evaluate(\"resnet\", classes)\n",
    "run_and_evaluate(\"vgg\", classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ba179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from torchcam.methods import GradCAM\n",
    "\n",
    "# ğŸ”¥ Grad-CAM çƒ­åŠ›å›¾å åŠ \n",
    "def overlay_heatmap(img: np.ndarray, cam: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n",
    "    cam_uint8 = np.uint8(255 * cam)\n",
    "    heatmap = cv2.applyColorMap(cam_uint8, cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    if img.max() > 1.0:\n",
    "        img = np.float32(img) / 255\n",
    "    if img.shape[:2] != heatmap.shape[:2]:\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    overlayed = heatmap * alpha + img\n",
    "    overlayed = overlayed / np.max(overlayed)\n",
    "    return np.uint8(255 * overlayed)\n",
    "\n",
    "# ğŸ¨ è¿˜åŸå›¾åƒé¢œè‰²\n",
    "def unnormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "# ğŸ§  Grad-CAMï¼šä¸ºæ¯ä¸€ç±»è¾“å‡ºè‡³å°‘ä¸€å¼ çƒ­åŠ›å›¾\n",
    "def apply_gradcam_all_classes(model, device, dataloader, model_name=\"resnet\", save_dir=\"gradcam_outputs\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    # âœ… Grad-CAM ç›®æ ‡å±‚\n",
    "    if model_name == \"resnet\":\n",
    "        target_layer = model.layer4[-1].conv2\n",
    "    elif model_name == \"vgg\":\n",
    "        target_layer = model.features[-1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model\")\n",
    "\n",
    "    cam_extractor = GradCAM(model, target_layer=target_layer)\n",
    "\n",
    "    seen_classes = defaultdict(int)\n",
    "    total_target_classes = 15\n",
    "    class_id_to_name = [\n",
    "        \"Agriculture\", \"Airport\", \"Beach\", \"City\", \"Desert\", \"Forest\", \"Grassland\", \"Highway\",\n",
    "        \"Lake\", \"Mountain\", \"Parking\", \"Port\", \"Railway\", \"Residential\", \"River\"\n",
    "    ]\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        for i in range(inputs.shape[0]):\n",
    "            label = labels[i].item()\n",
    "            if seen_classes[label] >= 1:\n",
    "                continue  # å·²ç»ç”Ÿæˆäº†è¯¥ç±»çš„å¯è§†åŒ–å›¾ï¼Œè·³è¿‡\n",
    "\n",
    "            img_tensor = inputs[i].unsqueeze(0)\n",
    "            output = model(img_tensor)\n",
    "            class_idx = torch.argmax(output).item()\n",
    "\n",
    "            # ç”Ÿæˆ Grad-CAM\n",
    "            cam_tensor = cam_extractor(class_idx=class_idx, scores=output)[0]\n",
    "            cam = cam_tensor.cpu().numpy()\n",
    "            if cam.ndim == 3:\n",
    "                cam = cam[0]\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "\n",
    "            # è¿˜åŸåŸå§‹å›¾åƒ\n",
    "            unnorm_img_tensor = unnormalize(img_tensor.squeeze(0).cpu(),\n",
    "                                            mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "            raw_image = np.clip(unnorm_img_tensor.permute(1, 2, 0).numpy(), 0, 1)\n",
    "\n",
    "            cam = cv2.resize(cam, (raw_image.shape[1], raw_image.shape[0]))\n",
    "            result = overlay_heatmap(raw_image, cam)\n",
    "            result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # ä¿å­˜å›¾åƒ\n",
    "            orig_img = np.uint8(raw_image * 255)\n",
    "            orig_bgr = cv2.cvtColor(orig_img, cv2.COLOR_RGB2BGR)\n",
    "            result_bgr = cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            cname = class_id_to_name[label]\n",
    "            cv2.imwrite(os.path.join(save_dir, f\"{cname}_original.png\"), orig_bgr)\n",
    "            cv2.imwrite(os.path.join(save_dir, f\"{cname}_gradcam.png\"), result_bgr)\n",
    "            cv2.imwrite(os.path.join(save_dir, f\"{cname}_compare.png\"), np.hstack((orig_bgr, result_bgr)))\n",
    "\n",
    "            # å¹¶æ’å¯è§†åŒ–\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            axs[0].imshow(orig_img)\n",
    "            axs[0].set_title(f\"{cname} - Original\")\n",
    "            axs[0].axis('off')\n",
    "            axs[1].imshow(result_rgb)\n",
    "            axs[1].set_title(f\"{cname} - GradCAM (Pred: {class_id_to_name[class_idx]})\")\n",
    "            axs[1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            seen_classes[label] += 1\n",
    "\n",
    "        if len(seen_classes) >= total_target_classes:\n",
    "            print(\"âœ… å·²ä¸ºæ‰€æœ‰ç±»åˆ«ç”Ÿæˆ Grad-CAM å¯è§†åŒ–ã€‚\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0021b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_gradcam_all_classes(model, device, test_loader, model_name=\"resnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchcam.methods import GradCAM\n",
    "# from torchvision.transforms.functional import to_pil_image\n",
    "# from torchcam.utils import overlay_mask\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # è‡ªåŠ¨é€‰æ‹©é€‚é…çš„ target layer\n",
    "# target_layer = \"layer4\" if \"resnet\" in config[\"model_name\"] else \"features.29\"\n",
    "# cam_extractor = GradCAM(model, target_layer=target_layer)\n",
    "\n",
    "# # è¾“å‡ºå¤šå¼ éªŒè¯é›†ä¸­å›¾åƒçš„ Grad-CAM å¯è§†åŒ–\n",
    "# n_samples = 8  # å¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "# print(f\"ğŸ¯ Grad-CAM å¯è§†åŒ–å‰ {n_samples} å¼ éªŒè¯å›¾åƒï¼š\")\n",
    "# count = 0\n",
    "\n",
    "# for i, (img, label) in enumerate(val_loader):\n",
    "#     if count >= n_samples:\n",
    "#         break\n",
    "\n",
    "#     img = img.to(device)\n",
    "#     label = label.to(device)\n",
    "\n",
    "#     output = model(img)\n",
    "#     class_idx = output.squeeze(0).argmax().item()\n",
    "\n",
    "#     # è·å–æ¿€æ´»å›¾\n",
    "#     activation_map = cam_extractor(class_idx, output)[0].cpu()\n",
    "\n",
    "#     # çƒ­åŠ›å›¾å åŠ \n",
    "#     result = overlay_mask(\n",
    "#         to_pil_image(img.squeeze().cpu().detach().clamp(0, 1)),\n",
    "#         to_pil_image(activation_map, mode='F'),\n",
    "#         alpha=0.5\n",
    "#     )\n",
    "\n",
    "#     # æ˜¾ç¤º\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     plt.title(f\"Predicted Class: {class_idx}\")\n",
    "#     plt.imshow(result)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "#     count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

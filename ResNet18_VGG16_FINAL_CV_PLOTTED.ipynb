{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00211924",
   "metadata": {},
   "source": [
    "### ğŸ“¦ ç¬¬ 0 æ ¼ï¼šå¯¼å…¥å¿…è¦çš„åº“\n",
    "å¯¼å…¥äº†å¦‚ `torch`, `cv2`, `numpy`, `matplotlib`, `tqdm`, `torchvision.transforms` ç­‰ç”¨äºå›¾åƒå¤„ç†ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒå¯è§†åŒ–æ‰€éœ€çš„åŸºç¡€åŒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # å¯¼å…¥åº“\n",
    "import cv2  # å¯¼å…¥åº“\n",
    "import torch  # å¯¼å…¥åº“\n",
    "import random  # å¯¼å…¥åº“\n",
    "import numpy as np  # å¯¼å…¥åº“\n",
    "import matplotlib.pyplot as plt  # å¯¼å…¥åº“\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d75154",
   "metadata": {},
   "source": [
    "### âš™ï¸ ç¬¬ 1 æ ¼ï¼šè¶…å‚æ•°é…ç½®\n",
    "å®šä¹‰æ¨¡å‹åç§°ã€è®­ç»ƒè½®æ•°ã€æ‰¹å¤§å°ã€å­¦ä¹ ç‡ã€æµ‹è¯•é›†æ¯”ä¾‹ç­‰è®­ç»ƒé…ç½®å‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ccc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¶…å‚æ•°è®¾ç½®\n",
    "config = {  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "    \"model_name\": \"resnet\",  # å¯é€‰ \"resnet\" æˆ– \"vgg\"\n",
    "    \"num_epochs\": 15,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"test_size\": 0.2,\n",
    "    \"sample_ratio\": 1.0\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_path = \"Aerial_Landscapes/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a4663",
   "metadata": {},
   "source": [
    "### ğŸ§  ç¬¬ 2 æ ¼ï¼šæ¨¡å‹åˆå§‹åŒ–å‡½æ•°\n",
    "æ ¹æ®ä¼ å…¥çš„æ¨¡å‹åï¼ˆresnet æˆ– vggï¼‰åˆå§‹åŒ–é¢„è®­ç»ƒæ¨¡å‹å¹¶æ›¿æ¢è¾“å‡ºå±‚ä»¥åŒ¹é…æ•°æ®é›†ç±»åˆ«æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfaf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, use_pretrained=True):\n",
    "    input_size = 224\n",
    "    if model_name == \"resnet\":\n",
    "        model = models.resnet18(pretrained=use_pretrained)  # åˆå§‹åŒ– ResNet æ¨¡å‹\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)  # ä½¿ç”¨ResNet18\n",
    "    elif model_name == \"vgg\":\n",
    "        model = models.vgg16(pretrained=use_pretrained)  # åˆå§‹åŒ– VGG æ¨¡å‹\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"ä¸æ”¯æŒçš„æ¨¡å‹åç§°ï¼Œè¯·é€‰æ‹© 'resnet' æˆ– 'vgg'\")\n",
    "    return model, input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858df1b",
   "metadata": {},
   "source": [
    "### ğŸ§± ç¬¬ 3 æ ¼ï¼šå›¾åƒé¢„å¤„ç†æµç¨‹\n",
    "å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•å›¾åƒæ‰€ä½¿ç”¨çš„å›¾åƒå¢å¼ºä¸å½’ä¸€åŒ–æ“ä½œï¼ŒåŒ…æ‹¬éšæœºè£å‰ªã€ç¼©æ”¾ã€æ ‡å‡†åŒ–ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(input_size=224):\n",
    "    train_transform = transforms.Compose([  # å®šä¹‰å›¾åƒé¢„å¤„ç†ç®¡é“\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    # === æ›¿æ¢åçš„ test_transform: åŠ å…¥æ¨¡ç³Š+å™ªå£°å¢å¼º ===\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),  # âœ… æ·»åŠ é«˜æ–¯æ¨¡ç³Š\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x + 0.05 * torch.randn_like(x)),    # âœ… æ·»åŠ é«˜æ–¯å™ªå£°\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# åŸå§‹ test_transform è¢«è¦†ç›–\n",
    "# test_transform = transforms.Compose([  # å®šä¹‰å›¾åƒé¢„å¤„ç†ç®¡é“\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… å®šä¹‰é®æŒ¡å‡½æ•°ï¼šå°†å›¾åƒä¸­å¿ƒåŒºåŸŸé®æˆé»‘è‰²ï¼Œæ¨¡æ‹Ÿé®æŒ¡æƒ…å†µ\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import ImageDraw\n",
    "\n",
    "class RandomOcclusion:\n",
    "    def __init__(self, size=(60, 60)):\n",
    "        self.size = size  # é®æŒ¡å—å°ºå¯¸\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # å‡è®¾ img æ˜¯ PIL Image\n",
    "        w, h = img.size\n",
    "        x0 = w // 2 - self.size[0] // 2\n",
    "        y0 = h // 2 - self.size[1] // 2\n",
    "        x1 = x0 + self.size[0]\n",
    "        y1 = y0 + self.size[1]\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=(0, 0, 0))\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe6003",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ§ª é²æ£’æ€§å®éªŒ 2ï¼šå›¾åƒé®æŒ¡æµ‹è¯•\n",
    "\n",
    "æˆ‘ä»¬åœ¨æµ‹è¯•é›† transform ä¸­åŠ å…¥ `RandomOcclusion` æ“ä½œï¼Œå³åœ¨å›¾åƒä¸­å¿ƒåŒºåŸŸæ·»åŠ é»‘è‰²é®æŒ¡å—ï¼Œç”¨äºæ¨¡æ‹Ÿç°å®ä¸­æ‘„åƒå¤´è¢«é®æŒ¡æˆ–å›¾åƒä¸å®Œæ•´çš„æƒ…å†µã€‚\n",
    "é€šè¿‡è¯„ä¼°æ¨¡å‹åœ¨é®æŒ¡æ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œå¯ä»¥æµ‹è¯•å…¶é²æ£’æ€§ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604147e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… åº”ç”¨é®æŒ¡å¢å¼º transformï¼ˆæµ‹è¯•æ—¶ä½¿ç”¨ï¼‰\n",
    "occluded_test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    RandomOcclusion(size=(60, 60)),                     # åŠ é®æŒ¡å—\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e50eb",
   "metadata": {},
   "source": [
    "### ğŸ—‚ï¸ ç¬¬ 4 æ ¼ï¼šè‡ªå®šä¹‰ PyTorch æ•°æ®é›†ç±»\n",
    "ç”¨äºå°†å›¾åƒå’Œæ ‡ç­¾ç»„åˆä¸º Dataset å¯¹è±¡ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ DataLoader åŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):  # è‡ªå®šä¹‰æ•°æ®é›†ç±»\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759792e1",
   "metadata": {},
   "source": [
    "### ğŸ§ª ç¬¬ 5 æ ¼ï¼šæ•°æ®åŠ è½½å’Œåˆ’åˆ†å‡½æ•°\n",
    "ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½å›¾åƒæ•°æ®ï¼Œå¹¶æŒ‰è®¾å®šæ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ŒåŒæ—¶è¿”å›ç±»åˆ«æ ‡ç­¾æ˜ å°„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564eb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(root_dir, test_size=0.2, sample_ratio=1.0):\n",
    "    classes = sorted(os.listdir(root_dir))\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    train_images, train_labels = [], []\n",
    "    test_images, test_labels = [], []\n",
    "\n",
    "    for cls_name in classes:\n",
    "        cls_path = os.path.join(root_dir, cls_name)\n",
    "        img_files = [os.path.join(cls_path, f) for f in os.listdir(cls_path) if f.endswith('.jpg')]\n",
    "        random.seed(42)\n",
    "        random.shuffle(img_files)\n",
    "        n_samples = int(len(img_files) * sample_ratio)\n",
    "        split = int(n_samples * (1 - test_size))\n",
    "        imgs = img_files[:n_samples]\n",
    "        train_imgs = imgs[:split]\n",
    "        test_imgs = imgs[split:]\n",
    "        train_images.extend([cv2.imread(p) for p in train_imgs])\n",
    "        train_labels.extend([class_to_idx[cls_name]] * len(train_imgs))\n",
    "        test_images.extend([cv2.imread(p) for p in test_imgs])\n",
    "        test_labels.extend([class_to_idx[cls_name]] * len(test_imgs))\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels), classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de969d42",
   "metadata": {},
   "source": [
    "### ğŸ” ç¬¬ 6 æ ¼ï¼šResNet/VGG è®­ç»ƒå‡½æ•°\n",
    "åŒ…æ‹¬è®­ç»ƒå¾ªç¯ã€Early Stoppingã€loss å’Œ acc çš„è®°å½•ï¼Œå¹¶åœ¨è®­ç»ƒå®Œæˆåç»˜åˆ¶å­¦ä¹ æ›²çº¿å›¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc41c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, device, train_loader, test_loader, criterion, optimizer,  # å®šä¹‰ ResNet/VGG é€šç”¨è®­ç»ƒå‡½æ•°\n",
    "                num_epochs=25, checkpoint_path='checkpoint.pth', patience=5):\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'test_loss': [], 'test_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for batch in train_loader_tqdm:\n",
    "            if early_stop:\n",
    "                print(f\"âš ï¸ æ—©åœè§¦å‘äºç¬¬ {epoch+1} è½®\")\n",
    "                break\n",
    "\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += torch.sum(preds == labels.data)\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        epoch_train_loss = train_loss / total_train\n",
    "        epoch_train_acc = correct_train.double() / total_train\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc.item())\n",
    "\n",
    "        model.eval()\n",
    "        test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_test += torch.sum(preds == labels.data)\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        epoch_test_loss = test_loss / total_test\n",
    "        epoch_test_acc = correct_test.double() / total_test\n",
    "        history['test_loss'].append(epoch_test_loss)\n",
    "        history['test_acc'].append(epoch_test_acc.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={epoch_train_acc:.4f}, Test Acc={epoch_test_acc:.4f}\")\n",
    "\n",
    "        if epoch_test_acc > best_acc:\n",
    "            best_acc = epoch_test_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                early_stop = True\n",
    "\n",
    "    return model, history\n",
    "    # âœ… æ·»åŠ è®­ç»ƒä¸éªŒè¯çš„æŸå¤±/å‡†ç¡®ç‡æ›²çº¿å›¾åƒ\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_loss'], label='Train Loss', c='red')\n",
    "    plt.plot(history['test_loss'], label='Val Loss', c='blue')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss Curve')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_acc'], label='Train Acc', c='orangered')\n",
    "    plt.plot(history['test_acc'], label='Val Acc', c='green')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.title('Accuracy Curve')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a002e",
   "metadata": {},
   "source": [
    "### ğŸ“Š ç¬¬ 7 æ ¼ï¼šè¯„ä¼°å‡½æ•°\n",
    "å¯¹æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°è¿›è¡Œè¯„ä¼°ï¼Œç”Ÿæˆé¢„æµ‹å€¼ä¸çœŸå®å€¼åˆ—è¡¨ï¼Œå¯ç”¨äºåç»­å¯è§†åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(xticks_rotation='vertical', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b9dfb",
   "metadata": {},
   "source": [
    "### ğŸ§ª ç¬¬ 8 æ ¼ï¼šåŠ è½½æ•°æ®é›†\n",
    "è°ƒç”¨æ•°æ®åŠ è½½å‡½æ•°å¹¶è·å–è®­ç»ƒå›¾åƒã€æµ‹è¯•å›¾åƒã€ç±»åˆ«æ ‡ç­¾ç­‰ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ef615",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels), classes = load_and_split_dataset(\n",
    "    dataset_path,\n",
    "    test_size=config[\"test_size\"],  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "    sample_ratio=config[\"sample_ratio\"]  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    ")\n",
    "\n",
    "model, input_size = initialize_model(\n",
    "    model_name=config[\"model_name\"],  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "    num_classes=len(classes),\n",
    "    use_pretrained=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "train_transform, test_transform = get_transforms(input_size)\n",
    "\n",
    "train_dataset = CustomDataset(train_images, train_labels, train_transform)\n",
    "test_dataset = CustomDataset(test_images, test_labels, test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "\n",
    "model, history = train_model(\n",
    "    model, device, train_loader, test_loader,\n",
    "    criterion, optimizer,\n",
    "    num_epochs=config[\"num_epochs\"],  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "print(\"Evaluation Metrics (Precision, Recall, F1-score):\\n\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "plt.figure(figsize=(12, 10))\n",
    "disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53773f0f",
   "metadata": {},
   "source": [
    "### â–¶ï¸ ç¬¬ 9 æ ¼ï¼šrun_and_evaluate å‡½æ•°\n",
    "è¿™æ˜¯ä¸€ä¸ªä¾¿æ·å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–æ¨¡å‹ã€è®­ç»ƒå¹¶è¯„ä¼°ç»“æœï¼Œé€‚ç”¨äºå¿«é€Ÿæ¯”è¾ƒå¤šä¸ªæ¨¡å‹è¡¨ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(model_name, classes):  # æ‰§è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°\n",
    "    (train_images, train_labels), (test_images, test_labels), _ = load_and_split_dataset(\n",
    "        dataset_path,\n",
    "        test_size=config[\"test_size\"],  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "        sample_ratio=config[\"sample_ratio\"]  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "    )\n",
    "\n",
    "    model, input_size = initialize_model(\n",
    "        model_name=model_name,\n",
    "        num_classes=len(classes),\n",
    "        use_pretrained=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_transform, test_transform = get_transforms(input_size)\n",
    "\n",
    "    train_dataset = CustomDataset(train_images, train_labels, train_transform)\n",
    "    test_dataset = CustomDataset(test_images, test_labels, test_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "\n",
    "    model, _ = train_model(\n",
    "        model, device, train_loader, test_loader,\n",
    "        criterion, optimizer,\n",
    "        num_epochs=config[\"num_epochs\"],  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "    print(f\"===== {model_name.upper()} Evaluation Metrics =====\\n\")\n",
    "    print(report)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for {model_name.upper()}\")\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” æ­£åœ¨æ¯”è¾ƒ ResNet18 ä¸ VGG16 çš„åˆ†ç±»æ€§èƒ½...\\n\")\n",
    "run_and_evaluate(\"resnet\", classes)  # æ‰§è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vgg_model(model, device, train_loader, val_loader, criterion, optimizer, num_epochs=10):  # å®šä¹‰ VGG çš„ç‹¬ç«‹è®­ç»ƒå‡½æ•°\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):  # åŠ  tqdm æ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accs.append(correct / total)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accs.append(val_correct / val_total)\n",
    "\n",
    "        print(f\"[VGG] Epoch {epoch+1}/{num_epochs} - Train Acc: {train_accs[-1]:.4f} - Val Acc: {val_accs[-1]:.4f}\")\n",
    "\n",
    "    import matplotlib.pyplot as plt  # å¯¼å…¥åº“\n",
    "    import numpy as np  # å¯¼å…¥åº“\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", c=\"red\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\", c=\"blue\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accs, label=\"Train Acc\", c=\"orangered\")\n",
    "    plt.plot(val_accs, label=\"Val Acc\", c=\"green\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103beaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = initialize_model(\"vgg\", num_classes=len(classes))[0].to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg_model.parameters(), lr=config[\"learning_rate\"])  # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°\n",
    "\n",
    "train_vgg_model(vgg_model, device, train_loader, test_loader, criterion, optimizer, num_epochs=config[\"num_epochs\"]) # è®¾ç½®æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ed68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ä½¿ç”¨ evaluate_model å‡½æ•°è¯„ä¼° VGG æ¨¡å‹åˆ†ç±»è¡¨ç°\n",
    "print(\"\\n==================== VGG Evaluation Metrics ====================\")\n",
    "evaluate_model(vgg_model, test_loader, device, class_names=classes)\n",
    "plt.title(f\"Confusion Matrix for VGG16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ba179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # å¯¼å…¥åº“\n",
    "import cv2  # å¯¼å…¥åº“\n",
    "import torch  # å¯¼å…¥åº“\n",
    "import numpy as np  # å¯¼å…¥åº“\n",
    "import matplotlib.pyplot as plt  # å¯¼å…¥åº“\n",
    "from collections import defaultdict\n",
    "from torchcam.methods import GradCAM\n",
    "\n",
    "def overlay_heatmap(img: np.ndarray, cam: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n",
    "    cam_uint8 = np.uint8(255 * cam)\n",
    "    heatmap = cv2.applyColorMap(cam_uint8, cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    if img.max() > 1.0:\n",
    "        img = np.float32(img) / 255\n",
    "    if img.shape[:2] != heatmap.shape[:2]:\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    overlayed = heatmap * alpha + img\n",
    "    overlayed = overlayed / np.max(overlayed)\n",
    "    return np.uint8(255 * overlayed)\n",
    "\n",
    "def unnormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def apply_gradcam_all_classes(model, device, dataloader, model_name=\"resnet\", save_dir=\"gradcam_outputs\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        target_layer = model.layer4[-1].conv2\n",
    "    elif model_name == \"vgg\":\n",
    "        target_layer = model.features[-1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model\")\n",
    "\n",
    "    cam_extractor = GradCAM(model, target_layer=target_layer)\n",
    "\n",
    "    seen_classes = defaultdict(int)\n",
    "    total_target_classes = 15\n",
    "    class_id_to_name = [\n",
    "        \"Agriculture\", \"Airport\", \"Beach\", \"City\", \"Desert\", \"Forest\", \"Grassland\", \"Highway\",\n",
    "        \"Lake\", \"Mountain\", \"Parking\", \"Port\", \"Railway\", \"Residential\", \"River\"\n",
    "    ]\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        for i in range(inputs.shape[0]):\n",
    "            label = labels[i].item()\n",
    "            if seen_classes[label] >= 1:\n",
    "                continue \n",
    "\n",
    "            img_tensor = inputs[i].unsqueeze(0)\n",
    "            output = model(img_tensor)\n",
    "            class_idx = torch.argmax(output).item()\n",
    "\n",
    "            cam_tensor = cam_extractor(class_idx=class_idx, scores=output)[0]\n",
    "            cam = cam_tensor.cpu().numpy()\n",
    "            if cam.ndim == 3:\n",
    "                cam = cam[0]\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "\n",
    "            unnorm_img_tensor = unnormalize(img_tensor.squeeze(0).cpu(),\n",
    "                                            mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])\n",
    "            raw_image = np.clip(unnorm_img_tensor.permute(1, 2, 0).numpy(), 0, 1)\n",
    "\n",
    "            cam = cv2.resize(cam, (raw_image.shape[1], raw_image.shape[0]))\n",
    "            result = overlay_heatmap(raw_image, cam)\n",
    "            result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            orig_img = np.uint8(raw_image * 255)\n",
    "            orig_bgr = cv2.cvtColor(orig_img, cv2.COLOR_RGB2BGR)\n",
    "            result_bgr = cv2.cvtColor(result_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            cname = class_id_to_name[label]\n",
    "            cv2.imwrite(os.path.join(save_dir, f\"{cname}_original.png\"), orig_bgr)\n",
    "            cv2.imwrite(os.path.join(save_dir, f\"{cname}_gradcam.png\"), result_bgr)\n",
    "            cv2.imwrite(os.path.join(save_dir, f\"{cname}_compare.png\"), np.hstack((orig_bgr, result_bgr)))\n",
    "\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            axs[0].imshow(orig_img)\n",
    "            axs[0].set_title(f\"{cname} - Original\")\n",
    "            axs[0].axis('off')\n",
    "            axs[1].imshow(result_rgb)\n",
    "            axs[1].set_title(f\"{cname} - GradCAM (Pred: {class_id_to_name[class_idx]})\")\n",
    "            axs[1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            seen_classes[label] += 1\n",
    "\n",
    "        if len(seen_classes) >= total_target_classes:\n",
    "            print(\"âœ… å·²ä¸ºæ‰€æœ‰ç±»åˆ«ç”Ÿæˆ Grad-CAM å¯è§†åŒ–ã€‚\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0021b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_gradcam_all_classes(model, device, test_loader, model_name=\"resnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429cf6f",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ§ª æ¨¡å‹é²æ£’æ€§å¯¹æ¯”è¯„ä¼°ï¼šResNet vs VGG\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ `occluded_test_transform`ï¼ˆä¸­å¿ƒé®æŒ¡ + å½’ä¸€åŒ–ï¼‰å¯¹ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæ¯”è¾ƒå®ƒä»¬åœ¨é®æŒ¡å›¾åƒä¸Šçš„åˆ†ç±»æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰\n",
    "- ç²¾ç¡®ç‡ / å¬å›ç‡ / F1 åˆ†æ•°ï¼ˆPrecision / Recall / F1-scoreï¼‰\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªå®éªŒå¯ä»¥è§‚å¯Ÿä¸åŒæ¶æ„çš„é²æ£’æ€§å·®å¼‚ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… æ„å»ºé®æŒ¡æ•°æ®åŠ è½½å™¨\n",
    "occluded_dataset = CustomDataset(test_images, test_labels, transform=occluded_test_transform)\n",
    "occluded_loader = DataLoader(occluded_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# âœ… å¯¹ ResNet æ¨¡å‹åœ¨é®æŒ¡æ•°æ®ä¸Šè¯„ä¼°\n",
    "print(\"ğŸ” ResNet under Occlusion\")\n",
    "evaluate_model(resnet_model, occluded_loader, device, class_names=classes)\n",
    "\n",
    "# âœ… å¯¹ VGG æ¨¡å‹åœ¨é®æŒ¡æ•°æ®ä¸Šè¯„ä¼°\n",
    "print(\"\\nğŸ” VGG under Occlusion\")\n",
    "evaluate_model(vgg_model, occluded_loader, device, class_names=classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d87c6",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ” äº¤å‰éªŒè¯ï¼ˆCross-Validationï¼‰æ”¯æŒ\n",
    "\n",
    "ä¸ºäº†å¢å¼ºæ¨¡å‹æ€§èƒ½è¯„ä¼°çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **5-fold äº¤å‰éªŒè¯ï¼ˆ5-Fold CVï¼‰**ã€‚\n",
    "- å°†æ•´ä¸ªè®­ç»ƒæ•°æ®åˆ’åˆ†ä¸º 5 ä¸ªå­é›†ï¼ˆfoldsï¼‰\n",
    "- æ¯æ¬¡é€‰æ‹©å…¶ä¸­ä¸€ä¸ªä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä»– 4 ä¸ªä½œä¸ºè®­ç»ƒé›†\n",
    "- è®­ç»ƒ 5 æ¬¡ï¼Œè®¡ç®—å¹³å‡å‡†ç¡®ç‡å’Œ F1-score\n",
    "\n",
    "è¿™é¿å…äº†å•ä¸€è®­ç»ƒ/éªŒè¯åˆ’åˆ†å¯¼è‡´çš„å¶ç„¶åå·®ï¼Œæå‡æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93fe44",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ“ˆ æ¯æŠ˜è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ï¼ˆLoss & Accuracyï¼‰\n",
    "\n",
    "ä¸ºäº†è§‚å¯Ÿæ¨¡å‹åœ¨æ¯ä¸€æŠ˜è®­ç»ƒ/éªŒè¯è¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬è®°å½•äº†æ¯ä¸€æŠ˜çš„è®­ç»ƒä¸éªŒè¯ Loss/Accuracy æ›²çº¿ã€‚\n",
    "è¿™æœ‰åŠ©äºåˆ¤æ–­ï¼š\n",
    "- æ¨¡å‹æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ\n",
    "- ä¸åŒæŠ˜ä¹‹é—´è®­ç»ƒè¿‡ç¨‹æ˜¯å¦ç¨³å®šä¸€è‡´\n",
    "- æ¨¡å‹åœ¨å°‘é‡æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ˜¯å¦è‰¯å¥½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_list, f1_list = [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels)):\n",
    "    print(f\"ğŸ“‚ Fold {fold+1}/5\")\n",
    "    \n",
    "    X_train, y_train = image_paths[train_idx], labels[train_idx]\n",
    "    X_val, y_val = image_paths[val_idx], labels[val_idx]\n",
    "\n",
    "    train_ds = CustomDataset(X_train, y_train, transform=train_transform)\n",
    "    val_ds = CustomDataset(X_val, y_val, transform=test_transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = initialize_model(\"vgg\", num_classes=len(classes))[0].to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    # æ¯æŠ˜éƒ½è®°å½•æ›²çº¿\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        history['train_loss'].append(running_loss / len(train_loader))\n",
    "        history['train_acc'].append(correct / total)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        history['test_loss'].append(val_loss / len(val_loader))\n",
    "        history['test_acc'].append(val_correct / val_total)\n",
    "\n",
    "    # ç»˜åˆ¶æ¯æŠ˜è®­ç»ƒ/éªŒè¯æ›²çº¿\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['train_loss'], label='Train Loss', c='red')\n",
    "    plt.plot(history['test_loss'], label='Val Loss', c='blue')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold+1} - Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['train_acc'], label='Train Acc', c='orangered')\n",
    "    plt.plot(history['test_acc'], label='Val Acc', c='green')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title(f'Fold {fold+1} - Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # è®°å½•è¯„ä¼°æŒ‡æ ‡\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    print(f\"Fold {fold+1} Accuracy: {acc:.4f}, F1-score: {f1:.4f}\\n\")\n",
    "\n",
    "print(\"âœ… å¹³å‡å‡†ç¡®ç‡:\", np.mean(acc_list))\n",
    "print(\"âœ… å¹³å‡åŠ æƒF1åˆ†æ•°:\", np.mean(f1_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
